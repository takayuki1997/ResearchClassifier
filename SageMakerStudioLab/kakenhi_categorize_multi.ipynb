{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takayuki1997/ResearchClassifier/blob/main/SageMakerStudioLab/kakenhi_categorize_multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrgegdDZjf8E"
      },
      "source": [
        "# 学術分野の分類（複数）\n",
        "## kakenhi_categorize_multi.ipynb\n",
        "kakenhi_fine_tuning.ipynbでファインチューニングにより作成したモデルを用いて、新たな複数の学術文書（実際には科研費の概要テキスト）を11の大区分に分類するプログラム。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6moZnLFkFwr"
      },
      "source": [
        "### ライブラリのインストール\n",
        "SageMakerでは初回だけで良い（Google colaboratoryでは毎回）  \n",
        "ライブラリTransformers、およびnlpをインストールします。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "FKFEr6d4pUCI"
      },
      "outputs": [],
      "source": [
        "# pipのアップデート（たまに走らせても良いかも）\n",
        "!pip list\n",
        "!python -m pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qg6t5nnBjqs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# ライブラリのインストール\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install nlp\n",
        "!pip install datasets\n",
        "!pip install fugashi\n",
        "!pip install ipadic\n",
        "!pip install unidic-lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcHOX9LyZc2g"
      },
      "source": [
        "### Google ドライブとの連携  \n",
        "以下のコードを実行し、認証コードを使用してGoogle ドライブをマウントします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h7BA67Ed5wT",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# SageMakerでは不要\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjJ8R-f1pUCM"
      },
      "source": [
        "# 以下途中まで不要"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zliYGLC5g0h2"
      },
      "source": [
        "## ファインチューニング用データの読み込み＆書き出し\n",
        "### 科研費データの読み込み、整理\n",
        "科研費データベースからダウンロードしたcsvファイルを直接読む  \n",
        "必要なデータを取り出す。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPV3qCYs9STS",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 科研費データベースからダウンロードした未加工のcsvファイルを指定\n",
        "# 基盤BC　2019年度～2021年度の４年分（2018年度は”研究開始時の研究の概要”が無い。2022年度は後半の検証に用いる）\n",
        "open_original_csv = \"KibanBC_2019-2021.csv\"\n",
        "\n",
        "# data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "data_path = \"../data/\" # SageMaker\n",
        "\n",
        "# csvファイルを開く\n",
        "raw_data = pd.read_csv(data_path + open_original_csv) # dtype=\"object\"必要？\n",
        "\n",
        "# 読み込んだデータをチェック\n",
        "# raw_data.info()\n",
        "\n",
        "# 今後必要な行だけを取り出し、リネーム\n",
        "# kadai = raw_data[[\"研究課題/領域番号\", \"審査区分\", \"研究課題名\", \"研究開始時の研究の概要\", \"研究成果の概要\", \"研究実績の概要\", \"キーワード\"]]\n",
        "# kadai.columns = [\"ID\", \"ShoKubun\", \"Title\", \"Abst1\", \"Abst2\", \"Abst3\", \"Keyword\"]\n",
        "kadai = raw_data[[\"研究課題/領域番号\", \"審査区分\", \"研究開始時の研究の概要\"]]\n",
        "kadai.columns = [\"ID\", \"ShoKubun\", \"Abst\"]\n",
        "\n",
        "# 課題番号の重複を確認。課題番号でソートする。\n",
        "kadai[\"ID\"].duplicated().any()\n",
        "# kadai = kadai.set_index(\"ID\") # IDをインデックスに設定するコード\n",
        "kadai = kadai.sort_values(\"ID\")\n",
        "kadai.reset_index(inplace=True, drop=True)\n",
        "\n",
        "# Abstの各項目が英語だけの場合、ブランクに入れ替える\n",
        "# kadai.Abst1[~kadai[\"Title\"].str.contains(r'[ぁ-んァ-ン]', na=True)] = \"\"\n",
        "# kadai.Abst2[~kadai[\"Abst1\"].str.contains(r'[ぁ-んァ-ン]', na=True)] = \"\"\n",
        "# kadai.Abst3[~kadai[\"Abst2\"].str.contains(r'[ぁ-んァ-ン]', na=True)] = \"\"\n",
        "# kadai.Abst1[~kadai[\"Abst3\"].str.contains(r'[ぁ-んァ-ン]', na=True)] = \"\"\n",
        "\n",
        "# 研究開始時の研究の概要、研究成果の概要、研究実績の概要、キーワードを結合\n",
        "# kadai['Abst'] = kadai['Title'].fillna('') + kadai['Abst1'].fillna('') + kadai['Abst2'].fillna('') + kadai['Abst3'].fillna('') + kadai['Keyword'] # t01\n",
        "# kadai['Abst'] = kadai['Title'].fillna('') + '。' + kadai['Keyword'] + '。' + kadai['Abst1'].fillna('') + kadai['Abst2'].fillna('') + kadai['Abst3'].fillna('') # t02\n",
        "\n",
        "\n",
        "\n",
        "# Abstが空欄の課題を削除\n",
        "print(\"オリジナルの課題数： %5d\" % len(kadai))\n",
        "print(\"概要が空白の課題数： %5d\" % len(kadai[kadai[\"Abst\"].isna()]))\n",
        "kadai = kadai.dropna(subset=[\"Abst\"])\n",
        "print(\"空白を除いた課題数： %5d\\n\" % len(kadai))\n",
        "\n",
        "# Abst中の改行コード、全角スペースを削除\n",
        "kadai = kadai.replace('\\r', '', regex=True) # Carriage Return(MacOS9) \\r\\n for Windows\n",
        "kadai = kadai.replace('\\n', '', regex=True) # Line Feed（Unix MacOSX）\n",
        "# kadai = kadai.replace(' ', '', regex=True) # 半角スペースは英語があるので削除しない\n",
        "kadai = kadai.replace('　', '', regex=True) # 全角スペース\n",
        "\n",
        "# Abstが英語のみの課題を削除\n",
        "num_jpen = len(kadai)\n",
        "kadai = kadai[kadai[\"Abst\"].str.contains(r'[ぁ-んァ-ン]')]\n",
        "num_jp   = len(kadai)\n",
        "print(\"日本語＋英語： %5d\" % num_jpen)\n",
        "print(\"英語　　　　： %5d\" % (num_jpen - num_jp))\n",
        "print(\"日本語　　　： %5d\\n\" % num_jp)\n",
        "\n",
        "# 小区分が設定されていない課題を削除（旧分類、特設分野）\n",
        "aaa = len(kadai)\n",
        "kadai = kadai.dropna(subset=[\"ShoKubun\"])\n",
        "print(\"小区分がブランク： %5d\" % (aaa - len(kadai)))\n",
        "print(\"小区分の設定あり： %5d\\n\" % len(kadai))\n",
        "\n",
        "# 小区分の文字列の数字部分だけを取り出す\n",
        "kadai[\"ShoKubun\"] = kadai[\"ShoKubun\"].str[3:8]\n",
        "kadai = kadai.astype({\"ShoKubun\": int})\n",
        "\n",
        "kadai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LADy70wOgyXg"
      },
      "source": [
        "### 整理した科研費データの保存\n",
        "審査区分データを読み込み、小区分番号を参照して結合  \n",
        "トレーニングデータと、テストデータに分けて保存する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIyvN2MT4Unl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 科研費の審査区分表データのcsvファイル\n",
        "open_kubun_csv = \"KubunTable.csv\"\n",
        "# data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "data_path = \"./\" # sagemaker\n",
        "\n",
        "# 書き出し用CSVファイル名\n",
        "train_csv = data_path + \"kadai_train.csv\"\n",
        "test_csv  = data_path + \"kadai_test.csv\"\n",
        "\n",
        "# ============================================================================\n",
        "\n",
        "# 審査区分テーブルのロード\n",
        "kubun_table = pd.read_csv(data_path + open_kubun_csv, encoding=\"cp932\")\n",
        "kubun_table = kubun_table[[\"tabDai\", \"tabSho\"]]\n",
        "\n",
        "# 審査区分表の重複を削除（一つの小区分が２つまたは３つの『中区分』に所属することに由来する）\n",
        "print(\"重複削除前の項目数： %3d\" % len(kubun_table))\n",
        "kubun_table = kubun_table.drop_duplicates()\n",
        "print(\"重複削除後の項目数： %3d\\n\" % len(kubun_table))\n",
        "\n",
        "## 中区分への変換\n",
        "## mergeを用いて、審査区分表のデータと突合\n",
        "#print(\"統合前のデータ数： %5d\" % len(kadai))\n",
        "#kadaiChu = pd.merge(kadai, kubun_table, left_on='ShoKubun', right_on='tabSho')\n",
        "#kadaiChu = kadaiChu[[\"Abst\", \"tabChu\", \"ID\", \"ShoKubun\"]]\n",
        "#print(\"統合したデータ数： %5d\\n\" % len(kadaiChu))\n",
        "\n",
        "# 大区分への変換\n",
        "# mergeを用いて、審査区分表のデータと突合\n",
        "print(\"統合前のデータ数： %5d\" % len(kadai))\n",
        "kadaiDai = pd.merge(kadai, kubun_table, left_on='ShoKubun', right_on='tabSho')\n",
        "kadaiDai = kadaiDai[[\"Abst\", \"tabDai\", \"ID\", \"ShoKubun\"]]\n",
        "print(\"統合したデータ数： %5d\" % len(kadaiDai))\n",
        "\n",
        "# 訓練用とテスト用に分割 層化\n",
        "kadai_train, kadai_test =  train_test_split(kadaiDai, shuffle=True, stratify = kadaiDai[\"tabDai\"].tolist())\n",
        "print(\"トレーニングデータ数： %5d\"   % len(kadai_train))\n",
        "print(\"テストデータ数　　　： %5d\\n\" % len(kadai_test))\n",
        "\n",
        "\n",
        "\n",
        "## 訓練用とテスト用に分割 層化\n",
        "#kadai_train, kadai_test =  train_test_split(kadaiChu, shuffle=True, stratify = kadaiChu[\"tabChu\"].tolist())\n",
        "#print(\"トレーニングデータ数： %5d\" % len(kadai_train))\n",
        "#print(\"テストデータ数　　　： %5d\\n\" % len(kadai_test))\n",
        "\n",
        "# 課題番号→小区分→中区分を基準にソート（計算的には不要だが、人間用にソートしておく）\n",
        "kadai_train = kadai_train.sort_values([\"tabDai\", \"ShoKubun\", \"ID\"])\n",
        "kadai_test  = kadai_test.sort_values ([\"tabDai\", \"ShoKubun\", \"ID\"])\n",
        "\n",
        "# ソート用に残していた課題番号（ID）行を削除\n",
        "#kadai_train = kadai_train.drop(['ID'], axis=1)\n",
        "#kadai_test  = kadai_test.drop (['ID'], axis=1)\n",
        "kadai_train = kadai_train.drop(['ID', 'ShoKubun'], axis=1)\n",
        "kadai_test  = kadai_test.drop (['ID', 'ShoKubun'], axis=1)\n",
        "\n",
        "# csvとして書き出し\n",
        "kadai_train.to_csv(train_csv, header=False, index=False)\n",
        "kadai_test.to_csv (test_csv,  header=False, index=False)\n",
        "\n",
        "print(\"Saved\\n %s\\n %s\\n\" % (train_csv, test_csv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsgQNMJxpBnW"
      },
      "source": [
        "## ファインチューニングの実施\n",
        "### モデルとTokenizerの読み込み\n",
        "日本語の事前学習済みモデルと、これと紐づいたTokenizerを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R0HK29fHrf3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
        "\n",
        "sc_model = BertForSequenceClassification.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", num_labels=11) # 大区分は11\n",
        "sc_model.cuda() # GPUを使う\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
        "\n",
        "print(\"Finish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWCmm2TjqToE"
      },
      "source": [
        "### データセットの読み込み\n",
        "さきほど作成した科研費データ（training,test）を読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfEnNpv9HuXI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def tokenize(batch):\n",
        "    # return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=128)\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512)\n",
        "\n",
        "data_path = \"./\"\n",
        "\n",
        "train_data = load_dataset(\"csv\", data_files=data_path+\"news_train.csv\", column_names=[\"text\", \"label\"], split=\"train\")\n",
        "train_data = train_data.map(tokenize, batched=True, batch_size=len(train_data))\n",
        "train_data.set_format(\"torch\", columns=[\"input_ids\", \"label\"])\n",
        "\n",
        "test_data = load_dataset(\"csv\", data_files=data_path+\"news_test.csv\", column_names=[\"text\", \"label\"], split=\"train\")\n",
        "test_data = test_data.map(tokenize, batched=True, batch_size=len(test_data))\n",
        "test_data.set_format(\"torch\", columns=[\"input_ids\", \"label\"])\n",
        "\n",
        "print(\"Finish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y6Fcqmy2rG2"
      },
      "source": [
        "## 評価用の関数\n",
        "`sklearn.metrics`を使用し、モデルを評価するための関数を定義します。  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plAZjdkG0FdV",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(result):\n",
        "    labels = result.label_ids\n",
        "    preds = result.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "    }\n",
        "\n",
        "print(\"Finish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjLqAVy7z0T3"
      },
      "source": [
        "### Trainerの設定\n",
        "Trainerクラス、およびTrainingArgumentsクラスを使用して、訓練を行うTrainerの設定を行います。  \n",
        "https://huggingface.co/transformers/main_classes/trainer.html  \n",
        "https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhaexaAOI3kV",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./results\",\n",
        "    num_train_epochs = 2,\n",
        "    per_device_train_batch_size = 8,\n",
        "    per_device_eval_batch_size = 32,\n",
        "    warmup_steps = 500,  # 学習係数が0からこのステップ数で上昇\n",
        "    weight_decay = 0.01,  # 重みの減衰率\n",
        "    # evaluate_during_training = True,  # ここの記述はバージョンによっては必要ありません\n",
        "    logging_dir = \"./logs\",\n",
        "    save_total_limit=1,  # limit the total amount of checkpoints. Deletes the older checkpoints.\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = sc_model,\n",
        "    args = training_args,\n",
        "    compute_metrics = compute_metrics,\n",
        "    train_dataset = train_data,\n",
        "    eval_dataset = test_data,\n",
        ")\n",
        "\n",
        "print(\"Finish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0F5nXKpSCnS"
      },
      "source": [
        "### モデルの訓練\n",
        "設定に基づきファインチューニングを行います。  \n",
        "40分程度かかる。 colaboratory 32分かかった　sagemaker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29fkN4UcI4jm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "\n",
        "print(\"Finish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c76zhkQVS2xZ"
      },
      "source": [
        "### モデルの評価\n",
        "Trainerの`evaluate()`メソッドによりモデルを評価します。  \n",
        "２分程度かかる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIgke21zI6l_",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()\n",
        "\n",
        "print(\"Finish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EFwqzLRUhaB"
      },
      "source": [
        "### TensorBoardによる結果の表示\n",
        "TensorBoardを使って、logsフォルダに格納された学習過程を表示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vv39tuDJq5n",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-BscHjHxs0H"
      },
      "source": [
        "### モデルの保存\n",
        "訓練済みのモデルを保存します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvwVcXuIyH7V",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# data_path = \"/content/drive/My Drive/bert_nlp/section_5/\"\n",
        "data_path = \"./\"\n",
        "\n",
        "sc_model.save_pretrained(data_path)\n",
        "tokenizer.save_pretrained(data_path)\n",
        "\n",
        "print(\"Finish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ec5bqsRpUCS"
      },
      "source": [
        "# ここまでは不要"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM6gUAojpUCS"
      },
      "source": [
        "## 複数の学術文書を分類\n",
        "学習済み（ファインチューニング済み）のモデルを読み込み、複数の学術文書（実際には科研費概要テキスト）を分類する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZuJCZBK0RJx"
      },
      "source": [
        "### モデルの読み込み\n",
        "保存済みのモデルを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWtcQRuP0X45",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
        "\n",
        "data_path = \"/content/drive/My Drive/ResearchClassifier/\" # Google colaboratory\n",
        "#data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "# data_path = \"./\" # SageMaker\n",
        "\n",
        "loaded_model = BertForSequenceClassification.from_pretrained(data_path)\n",
        "loaded_model.cuda() # GPU対応\n",
        "loaded_tokenizer = BertJapaneseTokenizer.from_pretrained(data_path)\n",
        "\n",
        "print(\"Finish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IFCjdI9pUCT"
      },
      "source": [
        "### 分類精度検証用の科研費データ読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "4LheLJKCpUCT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 科研費データベースからダウンロードした未加工のcsvファイルを指定\n",
        "open_original_csv = \"KibanBC_2024-2024.csv\" # 直近の１年（2024年）\n",
        "# open_original_csv = \"KibanC_2021_Original.csv\"\n",
        "# open_original_csv = \"KibanC_2022-2022.csv\" # 直近の１年（2022年）\n",
        "# open_original_csv = \"KibanC_2019-2020.csv\" # 2018は「研究開始時の研究の概要」が無い\n",
        "#data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "# data_path = \"../data/\"\n",
        "\n",
        "# csvファイルを開く\n",
        "raw_data2 = pd.read_csv(data_path + 'data/' + open_original_csv) # dtype=\"object\"必要？\n",
        "\n",
        "# 読み込んだデータをチェック\n",
        "# raw_data.info()\n",
        "\n",
        "\n",
        "raw_data2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "ulT9P4QwpUCT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 科研費データベースからダウンロードした未加工のcsvファイルを指定\n",
        "open_original_csv = \"KibanBC_2024-2024.csv\" # 直近の１年（2024年）\n",
        "# open_original_csv = \"KibanC_2021_Original.csv\"\n",
        "# open_original_csv = \"KibanC_2022-2022.csv\" # 直近の１年（2022年）\n",
        "# open_original_csv = \"KibanC_2019-2020.csv\" # 2018は「研究開始時の研究の概要」が無い\n",
        "#data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "# data_path = \"../data/\"\n",
        "\n",
        "# csvファイルを開く\n",
        "raw_data2 = pd.read_csv(data_path + 'data/' + open_original_csv) # dtype=\"object\"必要？\n",
        "\n",
        "# 読み込んだデータをチェック\n",
        "# raw_data.info()\n",
        "\n",
        "# 今後必要な行だけを取り出し、リネーム\n",
        "kadai2 = raw_data2[[\"研究課題/領域番号\", \"審査区分\", \"研究開始時の研究の概要\"]]\n",
        "kadai2.columns = [\"ID\", \"ShoKubun\", \"Abst\"]\n",
        "\n",
        "# 課題番号の重複を確認。課題番号でソートする。\n",
        "kadai2[\"ID\"].duplicated().any()\n",
        "# kadai2 = kadai2.set_index(\"ID\") # IDをインデックスに設定するコード\n",
        "kadai2 = kadai2.sort_values(\"ID\")\n",
        "\n",
        "# Abstが空欄の課題を削除\n",
        "print(\"オリジナルの課題数： %5d\" % len(kadai2))\n",
        "print(\"概要が空白の課題数： %5d\" % len(kadai2[kadai2[\"Abst\"].isna()]))\n",
        "kadai2 = kadai2.dropna(subset=[\"Abst\"])\n",
        "print(\"空白を除いた課題数： %5d\" % len(kadai2))\n",
        "\n",
        "# Abst中の改行コードを削除\n",
        "kadai2 = kadai2.replace('\\r', '', regex=True)\n",
        "kadai2 = kadai2.replace('\\n', '', regex=True)\n",
        "\n",
        "# Abstが英語のみの課題を削除\n",
        "num_jpen = len(kadai2)\n",
        "kadai2 = kadai2[kadai2[\"Abst\"].str.contains(r'[ぁ-んァ-ン]')]\n",
        "num_jp   = len(kadai2)\n",
        "print(\"日本語＋英語： %5d\" % num_jpen)\n",
        "print(\"英語　　　　： %5d\" % (num_jpen - num_jp))\n",
        "print(\"日本語　　　： %5d\" % num_jp)\n",
        "\n",
        "# kadai.to_csv(data_path + \"test1.csv\", encoding = \"cp932\")\n",
        "\n",
        "# 小区分が設定されていない課題を削除（旧分類、特設分野）\n",
        "aaa = len(kadai2)\n",
        "kadai2 = kadai2.dropna(subset=[\"ShoKubun\"])\n",
        "print(\"小区分がブランク： %5d\" % (aaa - len(kadai2)))\n",
        "print(\"小区分の設定あり： %5d\" % len(kadai2))\n",
        "\n",
        "# 小区分の文字列の数字部分だけを取り出す\n",
        "kadai2[\"ShoKubun\"] = kadai2[\"ShoKubun\"].str[3:8]\n",
        "kadai2 = kadai2.astype({\"ShoKubun\": int})\n",
        "\n",
        "print(\"Finish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgSrQJ0epUCT"
      },
      "source": [
        "### 大区分に変換"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "G1m2Hx38pUCT"
      },
      "outputs": [],
      "source": [
        "#import pandas as pd\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 科研費の審査区分表データのcsvファイル\n",
        "open_kubun_csv = \"KubunTable.csv\"\n",
        "# data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "# data_path = \"./\" # sagamaker\n",
        "\n",
        "# 審査区分テーブルのロード\n",
        "kubun_table = pd.read_csv(data_path + open_kubun_csv, encoding=\"cp932\")\n",
        "kubun_table = kubun_table[[\"tabDai\", \"tabSho\"]]\n",
        "\n",
        "# 審査区分表の重複を削除（一つの小区分が２つまたは３つの『中区分』に所属することに由来する）\n",
        "print(\"審査区分表の整理\")\n",
        "print(\"重複削除前の項目数： %3d\" % len(kubun_table))\n",
        "kubun_table = kubun_table.drop_duplicates()\n",
        "print(\"重複削除後の項目数： %3d\" % len(kubun_table))\n",
        "\n",
        "# 大区分への変換\n",
        "# mergeを用いて、審査区分表のデータと突合\n",
        "print(\"統合前のデータ数： %5d\" % len(kadai2))\n",
        "kadaiDai2 = pd.merge(kadai2, kubun_table, left_on='ShoKubun', right_on='tabSho')\n",
        "kadaiDai2 = kadaiDai2[[\"Abst\", \"tabDai\", \"ID\", \"ShoKubun\"]]\n",
        "print(\"統合したデータ数： %5d\" % len(kadaiDai2))\n",
        "\n",
        "kadaiDai2.info()\n",
        "\n",
        "show_num = 0\n",
        "kadaiDai2[\"Abst\"][show_num]\n",
        "\n",
        "print(\"Finish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8dEA1WkpUCT"
      },
      "source": [
        "### 分類精度を複数ファイルで確認\n",
        "３時間程度かかるかも  \n",
        "sagemakerでは40分程度"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob  # ファイルの取得に使用\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "results_binary = 'results'\n",
        "# data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "# data_path = \"./\" # sagemaker\n",
        "Daikubun = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\"]\n",
        "\n",
        "\n",
        "max_length = 512\n",
        "\n",
        "num_data = len(kadaiDai2)\n",
        "print(\"分類する課題数： %d\" % num_data)\n",
        "# num_data = 100\n",
        "num_category = len(Daikubun)\n",
        "# results = torch.zeros(num_data, num_category) # テンソルで変数を用意\n",
        "results = np.zeros((num_data, num_category)) # メモリーが足りないと言われるのでテンソルではなくnumpy arrayにしてみた\n",
        "\n",
        "for m in tqdm(range(num_data)):\n",
        "  words = loaded_tokenizer.tokenize(kadaiDai2[\"Abst\"][m])\n",
        "  word_ids = loaded_tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n",
        "  word_tensor = torch.tensor([word_ids[:max_length]])  # テンソルに変換\n",
        "\n",
        "  # y = loaded_model(word_tensor) # GPU未対応時の予測\n",
        "  y = loaded_model(word_tensor.cuda())  # GPU対応時の予測\n",
        "\n",
        "  # results[m,:] = y[0]\n",
        "  results[m,:] = y[0].cpu().detach().numpy()  # テンソルをCPUにコピーしてからNumPy配列に変換\n",
        "  # results[m,:] = y[0].detach().numpy() # テンソルをnumpy arrayに変換\n",
        "\n",
        "# 変数をとりあえずバイナリで保存\n",
        "np.save(data_path+results_binary, results) # 計算結果をとりあえずバイナリで保存\n",
        "\n",
        "print(results.shape)\n",
        "results"
      ],
      "metadata": {
        "id": "LrAWbxo22nSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "UdMFCAzYpUCU"
      },
      "outputs": [],
      "source": [
        "# 古い方\n",
        "import glob  # ファイルの取得に使用\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "results_binary = 'results'\n",
        "# data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "# data_path = \"./\" # sagemaker\n",
        "Daikubun = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\"]\n",
        "\n",
        "\n",
        "max_length = 512\n",
        "\n",
        "num_data = len(kadaiDai2)\n",
        "print(\"分類する課題数： %d\" % num_data)\n",
        "# num_data = 100\n",
        "num_category = len(Daikubun)\n",
        "# results = torch.zeros(num_data, num_category) # テンソルで変数を用意\n",
        "results = np.zeros((num_data, num_category)) # メモリーが足りないと言われるのでテンソルではなくnumpy arrayにしてみた\n",
        "\n",
        "for m in tqdm(range(num_data)):\n",
        "  words = loaded_tokenizer.tokenize(kadaiDai2[\"Abst\"][m])\n",
        "  word_ids = loaded_tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n",
        "  word_tensor = torch.tensor([word_ids[:max_length]])  # テンソルに変換\n",
        "\n",
        "  # y = loaded_model(word_tensor) # GPU未対応時の予測\n",
        "  y = loaded_model(word_tensor.cuda())  # GPU対応時の予測\n",
        "\n",
        "  # results[m,:] = y[0]\n",
        "  results[m,:] = y[0].detach().numpy() # テンソルをnumpy arrayに変換\n",
        "\n",
        "# 変数をとりあえずバイナリで保存\n",
        "np.save(data_path+results_binary, results) # 計算結果をとりあえずバイナリで保存\n",
        "\n",
        "print(results.shape)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBDoRRItpUCU"
      },
      "source": [
        "### ここで一度保存\n",
        "時間がかかる処理のあとなので、この段階で生成できたデータをファイルに保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "9Q701Jz5pUCU"
      },
      "outputs": [],
      "source": [
        "# コードが不十分\n",
        "# data_path = \"./\" # sagemaker\n",
        "Daikubun = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\"]\n",
        "\n",
        "results = np.load(data_path+results_binary+\".npy\")\n",
        "\n",
        "# 変数をとりあえずバイナリで保存\n",
        "#np.save(data_path+results_binary, results) # 計算結果をとりあえずバイナリで保存\n",
        "np.savez(data_path+\"categolization_results\", results, kadaiDai2, Daikubun)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2BLrKL3pUCU"
      },
      "source": [
        "分類精度を表示\n",
        "混同行列"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al4sgOdppUCU"
      },
      "source": [
        "## 分類結果を表示"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEMfRauMpUCV"
      },
      "source": [
        "### 分類精度を複数ファイルで確認\n",
        "confusion matrix （混同行列）を作成  \n",
        "「マルチラベリング」で対応する方法もありそう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "RYJckt0dpUCV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "show_num = 0\n",
        "\n",
        "results_binary = \"results\"\n",
        "# data_path = \"/content/drive/My Drive/bert_nlp/section_5/\" # Google colaboratory\n",
        "# data_path = \"./\" # sagemaker\n",
        "Daikubun = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\"]\n",
        "\n",
        "# num_category = len(Daikubun)\n",
        "\n",
        "results_2 = np.load(data_path+results_binary+\".npy\")\n",
        "\n",
        "#results_3 = np.load(data_path+\"categolization_results.npz\", allow_pickle=True)\n",
        "#results_2 = results_3['arr_0']\n",
        "#kadaiDai2 = results_3['arr_1']\n",
        "#Daikubun  = results_3['arr_2']\n",
        "\n",
        "print(\"ロードした推定確率データのサイズ %d %d\" % results_2.shape)\n",
        "\n",
        "num_category = len(Daikubun)\n",
        "\n",
        "results_2 = torch.tensor(results_2) # Softmax関数を使うためにテンソルに変換\n",
        "m = torch.nn.Softmax(dim=1) # Softmax関数で確率に変換\n",
        "results_2 = m(results_2)\n",
        "results_2 = results_2.numpy() # numpy arrayに戻す\n",
        "\n",
        "kadaiDai2['estimated'] = np.argmax(results_2, axis=1) # 最大要素のindexを返す（確率が最大の区分を返す）\n",
        "\n",
        "\n",
        "print(\"重複課題の削除前 %5d\" % len(kadaiDai2))\n",
        "kadaiDai3 = kadaiDai2.drop_duplicates(subset='ID', keep=False)\n",
        "print(\"重複課題の削除後 %5d\" % len(kadaiDai3))\n",
        "\n",
        "duplicated_data = kadaiDai3[kadaiDai3.duplicated(subset='ID', keep=False)]\n",
        "unique_id = duplicated_data['ID'].drop_duplicates()\n",
        "\n",
        "\n",
        "#\n",
        "#for kk in unique_id:\n",
        "#  dup_set = duplicated_data[duplicated_data['ID'] == kk]\n",
        "#  cat_est = dup_set['estimated'].tolist()[0]\n",
        "#  cat_real = dup_set['tabDai'].tolist()\n",
        "#\n",
        "#  if cat_est in cat_real:\n",
        "#    aaa = cat_real.index(cat_est)\n",
        "#    print(dup_set[:aaa])\n",
        "#  else:\n",
        "#    aaa = cat_est\n",
        "#\n",
        "#print(aaa)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# https://analysis-navi.com/?p=553\n",
        "confmat = confusion_matrix(kadaiDai3['tabDai'], kadaiDai3['estimated']) # 混同行列の取得。(true, predicted)の順番\n",
        "confmat = pd.DataFrame(confmat,columns=[\"pred_\" + str(l) for l in Daikubun], index=[\"act_\" + str(l) for l in Daikubun])\n",
        "# print(cm)\n",
        "\n",
        "\n",
        "#dup_idx_all = kadaiDai2.drop_duplicated(subset='ID',keep=False)\n",
        "\n",
        "#duplicated_data_all = kadaiDai2[dup_idx_all]\n",
        "#duplicated_data_all = duplicated_data_all[['ID', 'tabDai', 'estimated']]\n",
        "#print(duplicated_data_all.shape)\n",
        "#duplicated_data_all = duplicated_data_all.groupby('ID').count()\n",
        "\n",
        "#dup_idx_first = kadaiDai2[kadaiDai2.duplicated(subset='ID', keep='first')]\n",
        "#dup_idx_first = dup_idx_first.index\n",
        "\n",
        "#for kk in dup_idx_first:\n",
        "#  print(kk)\n",
        "\n",
        "\n",
        "#duplicated_data.to_csv(data_path+\"duplicated_mat.csv\")\n",
        "\n",
        "#kubun_estimated = np.argmax(results_2, axis=1)\n",
        "#kubun_real      = kadaiDai2[\"tabDai\"]\n",
        "#\n",
        "#kubun_results = pd.DataFrame(\n",
        "#    data={\n",
        "#        'estimated':kubun_estimated,\n",
        "#        'real':kubun_real\n",
        "#    }\n",
        "#)\n",
        "\n",
        "# 結果のグラフを表示\n",
        "plt.bar(Daikubun, results_2[show_num,:])\n",
        "print(kadaiDai3[\"Abst\"][show_num])\n",
        "print(kadaiDai3[\"ShoKubun\"][show_num])\n",
        "print(kadaiDai3['ID'][show_num])\n",
        "print(\"実際　大区分\" + Daikubun[kadaiDai3[\"tabDai\"][show_num]])\n",
        "print(\"推定　大区分\" + Daikubun[kadaiDai3[\"estimated\"][show_num]])\n",
        "print(int(results_2[show_num, kadaiDai3[\"estimated\"][show_num]]*100))\n",
        "#kadaiDai2 = kadaiDai2[[\"Abst\", \"tabDai\", \"ID\", \"ShoKubun\"]]\n",
        "\n",
        "#out_mat = kubun_results.value_counts(sort=False)\n",
        "#kubun_results.aggregate\n",
        "\n",
        "\n",
        "#out_mat.to_csv(data_path+\"result_mat.csv\")\n",
        "\n",
        "#dup_idx_first\n",
        "confmat.to_csv(data_path+\"confmat.csv\")\n",
        "confmat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "confmat = pd.read_csv(data_path+'confmat.csv', index_col=0)\n",
        "\n",
        "# ラベルのリスト（act_を除去）\n",
        "labels = [label.replace('act_', '') for label in confmat.index.tolist()]\n",
        "\n",
        "# 実際のラベルと予測されたラベルを展開\n",
        "actual = []\n",
        "predicted = []\n",
        "\n",
        "for actual_label, row in confmat.iterrows():\n",
        "    for pred_label, count in zip(row.index, row):\n",
        "        # act_とpred_を除去してラベルを統一\n",
        "        actual_clean = actual_label.replace('act_', '')\n",
        "        pred_clean = pred_label.replace('pred_', '')\n",
        "        actual.extend([actual_clean] * int(count))\n",
        "        predicted.extend([pred_clean] * int(count))\n",
        "\n",
        "# 評価指標の計算（zero_division=0を追加）\n",
        "accuracy = accuracy_score(actual, predicted)\n",
        "precision = precision_score(actual, predicted, labels=labels, average=None, zero_division=0)\n",
        "recall = recall_score(actual, predicted, labels=labels, average=None, zero_division=0)\n",
        "f1 = f1_score(actual, predicted, labels=labels, average=None, zero_division=0)\n",
        "\n",
        "# 結果を表示\n",
        "print(f\"全体の精度: {accuracy:.3f}\")\n",
        "print(\"\\n各クラスの評価指標:\")\n",
        "metrics_df = pd.DataFrame({\n",
        "    '適合率': precision,\n",
        "    '再現率': recall,\n",
        "    'F値': f1\n",
        "}, index=labels)\n",
        "print(metrics_df.round(2))\n"
      ],
      "metadata": {
        "id": "aAAXpsvA7kK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHIZJYZQpUCV"
      },
      "source": [
        "# 以下不要"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq2zZ99R3Hs7"
      },
      "source": [
        "### 学術分野の分類\n",
        "読み込んだモデルを使って学術分野を分類します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFOIjY511WVK",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import glob  # ファイルの取得に使用\n",
        "import os\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "Daikubun = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\"]\n",
        "\n",
        "# #@title String fields\n",
        "# sample_text = '' #@param {type:\"string\"}\n",
        "\n",
        "# sample_text = \"磁性ナノ粒子を用いた新しい診断治療技術に関する研究課題である。腫瘍等に集積させた磁性ナノ粒子に体外から比較的低い周波数の交流磁界を印加し、そのときに生じる磁気信号を検出することにより体内の画像診断が可能となる。また、より高い周波数の交流磁界を印加すると磁性ナノ粒子が発熱する。この発熱は癌の温熱治療（ハイパーサーミア）に利用することができる。交流磁界に対する磁性ナノ粒子の磁化応答（ダイナミクス）を解明し、これら診断治療の実用を目指す。\" # 1\n",
        "# sample_text = \"国際的な歴史的文字の共通検索実現およびオープンデータ化を目指して、奈文研・東京大学史料編纂所・国文学研究資料館・国立国語研究所・台湾中央研究院歴史語言研究所を中心に、京都大学人文学研究所や中国社会科学院歴史研究所等の研究者の参加も得ながら、国内・国外で各1回、研究会を開催した。そして、歴史的文字画像データ共通検索の基本コンセプトを、「開かれた」「対等」「継続的」として、このコンセプト基づいた「共通検索のためのるフレームワークの構築」作業に着手し、具体的な取り決めを決定した。この内容については、朝日新聞等でも報道された。詳細は『奈良文化財研究所紀要2019』に掲載予定である。また、研究の促進と参加誘発を目指して、奈良文化財研究所が公開している木簡関連の総合データベース「木簡庫」に検索した木簡情報（釈文・メタデータ等）をCSVファイルでダウンロードすることができる機能を追加し、オープンデータとした。これにより、利用者の目的に応じた木簡データのダウンロードおよびそれを活用しての研究や、さらにはそれぞれの関心に基づくデータベースの作成・公開も可能になった。この内容についても、朝日新聞・NHK等で報道された。文字に関する知識の集積作業として、あらたに気づきメモ約13000文字分（のべ）、観察記録シート約15000文字分（のべ）の情報を収集した。従来からの蓄積と合わせると、観察記録シートによる文字情報は木簡庫で公開している文字画像の約25％（テキストで公表している釈文文字数の約10％）に到達した。文字画像の切り出し・公開は1067字分行った。また、観察記録シートの手法を試験的に用い、中国晋代簡牘・韓国新羅木簡・日本平城宮木簡の比較を行い、それぞれの親和性を検討した研究を行い、成果を得た。この他、木簡の調査者との情報共有のためのワークショップを開催した。\" # 0\n",
        "# sample_text = \"本研究は、最新科学と考古学の有機的なコラボレーションによって、発掘によらない王陵級巨大古墳の調査研究の方法を確立することにある。巨大古墳は、発掘調査の禁止あるいは制限がかかっている。そのため、重要な歴史資料でありながら、内容が不明なことが多い。そうした現状を最新科学を用いて打破する。具体的にはミュオンという素粒子を用いてレントゲン写真のように古墳内部を透視する。さらに出土埴輪の形態学的分析＋化学分析・墳丘のレーザー測量を行う。主たるフィールドを吉備に置き、その調査成果を畿内の王陵と比較し、王陵級巨大古墳の構造分析を行う。\" # 0\n",
        "# sample_text = \"本研究は、低エネルギー動作を特徴とする断熱的量子磁束回路(AQFP)を用いた双方向演算が可能な可逆回路の学理を明らかにし、論理回路の熱力学的極限を超える究極の低消費エネルギー集積回路を実現する。これにより回路の消費エネルギーを半導体回路に対して６桁以上低減し、冷却電力を考慮しても十分な優位性を生み出す。本研究は可逆AQFP を中核技術とし、回路設計技術、新規可逆回路、プロセッサアーキテクチャ、磁性体を用いた位相シフトAQFP、3 次元集積回路技術を研究し、超省エネ集積回路の基盤技術を確立する。最終目標として100nW 以下の動作が可能な4b可逆AQFPプロセッサの実現を目指す。\" # 1\n",
        "# sample_text = \"ストリゴラクトンは根から分泌されて土壌中でAM菌との共生を促進する根圏シグナル物質である。AM菌共生は植物の陸上進出を可能にし、さらに陸上でのその後の繁栄を支えてきた。種子植物はSL受容体をもっており、SLは個体内で成長を調節する植物ホルモンとしても働き、養分吸収と成長のバランスを制御して植物の成長を最適化する。本研究では、植物がAM菌との共生関係を構築し、それに合わせて成長を調節する仕組みを進化させた道筋を分子レベルで理解することをめざす。本研究により、地球が緑の惑星となりえた理由の一端を明らかにすることができる。\" # 2\n",
        "# sample_text = \"発芽は、植物の一生において最も重要なイベントの1つである。様々な環境で初期生育を達成するために、植物は最適なタイミングで発芽する機構を進化させてきた。適切な発芽管理は農業的にも重要であり、特に種子を収穫する穀物植物においては直接収量に関わる。本研究では最近確立されたGWASシステムを用いて、世界的に栽培されている穀物植物であるイネにおいて温度依存的な発芽調節機構を明らかにすることを目的に研究を行う。\"\n",
        "# sample_text = \"食品中のトランス脂肪酸は、過剰摂取により心疾患のリスクを高めることが疫学調査によって報告され、世界中で注目を集めている。しかしながら、食事によって摂取されたトランス脂肪酸の体内動態は明らかにされておらず、心疾患とトランス脂肪酸の直接的な関連性は不明である。そこで本研究では、心疾患とトランス脂肪酸の因果関係の解明に資する基礎的なデータを蓄積することを目的とし、食品中に多く含まれる炭素数18、二重結合数1の13種類のトランス脂肪酸（trans-18:1）異性体をマウスに投与し、各臓器・組織中のtrans-18:1異性体を各種質量分析計で測定することで、トランス脂肪酸異性体の体内動態を明らかにする。\"\n",
        "# sample_text = \"本研究では、遅延時間が小さく、100％の回線利用率を達成する理想的なインターネット輻輳（ふくそう）制御手法を実現することを目的とする。具体的には、バッファブロートと呼ばれる、遅延時間が数秒クラスに増大する輻輳問題に着目する。従来のインターネット輻輳制御手法では用いられていないネットワーク計測手法を活用し、数学的理論に基づいて輻輳状態を正確に推定する。その結果を、新たな輻輳制御手法へ応用する。提案手法の有効性を数学的に保証すると共に、コンピュータシミュレーション及び実ネットワーク環境下での実験により実用性の検証を行う。\"\n",
        "# sample_text = \"現代の倫理学では、カントの倫理学は「義務論」に分類され、アリストテレス流の「徳倫理学」やベンサムに代表される「功利主義」と対比されるのが一般的である。しかしカント自身は晩年の『道徳の形而上学』において、義務論の枠内で徳論を展開している。本研究は、こうしたカントの徳倫理学の独自性を思想史的に遡って解明するとともに、それが倫理学の理論としてもつ強みについても明らかにする。\"\n",
        "\n",
        "# Abst中の改行コードを削除\n",
        "sample_text = sample_text.replace('\\r', '')\n",
        "sample_text = sample_text.replace('\\n', '')\n",
        "print(sample_text)\n",
        "\n",
        "\n",
        "max_length = 512\n",
        "words = loaded_tokenizer.tokenize(sample_text)\n",
        "word_ids = loaded_tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n",
        "print(len(word_ids))\n",
        "word_tensor = torch.tensor([word_ids[:max_length]])  # テンソルに変換\n",
        "\n",
        "\n",
        "x = word_tensor.cuda()  # GPU対応\n",
        "# x = word_tensor # GPU未対応==============================================\n",
        "y = loaded_model(x)  # 予測\n",
        "y = y[0]\n",
        "pred = y.argmax(-1)  # 最大値のインデックス\n",
        "out_put = Daikubun[pred]\n",
        "print(\"大区分\"+out_put)\n",
        "\n",
        "m = torch.nn.Softmax(dim=1) # Softmax関数で確率に変換\n",
        "y = m(y)\n",
        "yy = y.tolist()[0]\n",
        "yy = list(map(lambda x: int(x*100), yy))\n",
        "all_result = dict(zip(Daikubun, yy))\n",
        "print(all_result)\n",
        "\n",
        "# 結果のグラフを表示\n",
        "plt.bar(Daikubun, yy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "01_news_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}